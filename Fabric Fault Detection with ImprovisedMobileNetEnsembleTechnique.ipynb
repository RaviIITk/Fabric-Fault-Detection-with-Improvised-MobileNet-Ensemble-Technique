{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import PIL.Image as Image\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import pathlib\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Concatenate\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong path: /Users/ravitiwari/Desktop/Thesis/Image data/e0_Normal/.DS_Store\n",
      "Wrong path: /Users/ravitiwari/Desktop/Thesis/Image data/e1_hole/.DS_Store\n"
     ]
    }
   ],
   "source": [
    "\n",
    "IMG_SIZE = (224,224)\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "\n",
    "\n",
    "data_dir = pathlib.Path('/Users/ravitiwari/Desktop/Thesis/Image data')\n",
    "images=list(data_dir.glob('*/*.tif'))\n",
    "\n",
    "image_dic= {\n",
    "    'normal' : list(data_dir.glob(\"e0_Normal/*\")),\n",
    "    'hole':list(data_dir.glob(\"e1_hole/*\")),\n",
    "    'stain' : list(data_dir.glob(\"e2_Stain/*\")),\n",
    "    'net' :list(data_dir.glob(\"e3_Net/*\")),\n",
    "    'color' :list(data_dir.glob(\"e4_colour/*\")),\n",
    "    'crease': list(data_dir.glob(\"e5_Crease/*\")),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "labels = {\n",
    "    'normal':0,\n",
    "    'hole':1,\n",
    "    'stain':2,\n",
    "    'net':3,\n",
    "    'color':4,\n",
    "    'crease':5,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "X,y=[],[]\n",
    "\n",
    "for name, images in image_dic.items():\n",
    "    for image in images:\n",
    "        img = cv2.imread(str(image))\n",
    "        \n",
    "        \n",
    "        if img is None:\n",
    "            print('Wrong path:', image)\n",
    "        else:\n",
    "            resized = cv2.resize(img,IMG_SIZE)\n",
    "            X.append(resized)\n",
    "            y.append(labels[name])\n",
    "\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "y= np.array(y)\n",
    "X_train, X_test, y_train,y_test = train_test_split(X,y,random_state=42, shuffle=True,test_size=0.2)\n",
    "test_size = 0.5\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_test,y_test, test_size=0.5)\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "X_valid=X_valid/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "# base_model = tf.keras.applications.mobilenet.MobileNet(input_shape=(224,224,3),\n",
    "#                                                include_top=False,\n",
    "#                                                weights='imagenet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "def ensemble(x, inputs, classes):\n",
    "    seps  =[]\n",
    "    for i in range(classes):\n",
    "        res=tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        res = tf.keras.layers.Dense(128, activation='relu')(res)\n",
    "        res = tf.keras.layers.Dropout(0.4)(res)\n",
    "        res = tf.keras.layers.Dense(96, activation='relu')(res)\n",
    "        #res= tf.keras.layers.Dense(60,activation='relu',name='predictions'+str(i))(res)\n",
    "        #res = tf.keras.layers.Dense(64, activation='softmax')(res)\n",
    "        res = tf.keras.layers.Dense(32, activation='relu')(res)\n",
    "\n",
    "        seps.append(res)\n",
    "    out= Concatenate()(seps)\n",
    "    out = tf.keras.layers.Dense(6, activation='softmax')(out)\n",
    "    model = tf.keras.Model(inputs,out)\n",
    "    return model\n",
    "input_img = tf.keras.Input(IMG_SHAPE, name='input')\n",
    "model1= tf.keras.applications.mobilenet.MobileNet(input_tensor = input_img, include_top= False)\n",
    "model_mn = tf.keras.Sequential()\n",
    "\n",
    "for layers in model1.layers[:50]:\n",
    "    model_mn.add(layers)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x= model_mn(inputs)\n",
    "main_model= ensemble(x,inputs,6)\n",
    "\n",
    "for layers in main_model.layers[45:50]:\n",
    "    layers.trainable= False\n",
    "\n",
    "main_model.compile(optimizer = Adam(learning_rate=0.0005), loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True),metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "31/31 [==============================] - 44s 1s/step - loss: 3.0444 - categorical_accuracy: 0.1923 - val_loss: 1.9732 - val_categorical_accuracy: 0.2033\n",
      "Epoch 2/60\n",
      "31/31 [==============================] - 38s 1s/step - loss: 1.9415 - categorical_accuracy: 0.2682 - val_loss: 2.4225 - val_categorical_accuracy: 0.1867\n",
      "Epoch 3/60\n",
      "31/31 [==============================] - 40s 1s/step - loss: 1.7405 - categorical_accuracy: 0.3170 - val_loss: 2.0299 - val_categorical_accuracy: 0.2282\n",
      "Epoch 4/60\n",
      "31/31 [==============================] - 38s 1s/step - loss: 1.5438 - categorical_accuracy: 0.3690 - val_loss: 1.8544 - val_categorical_accuracy: 0.2199\n",
      "Epoch 5/60\n",
      "31/31 [==============================] - 38s 1s/step - loss: 1.3930 - categorical_accuracy: 0.4283 - val_loss: 1.6633 - val_categorical_accuracy: 0.3071\n",
      "Epoch 6/60\n",
      "31/31 [==============================] - 39s 1s/step - loss: 1.3022 - categorical_accuracy: 0.4782 - val_loss: 1.6128 - val_categorical_accuracy: 0.3568\n",
      "Epoch 7/60\n",
      "31/31 [==============================] - 40s 1s/step - loss: 1.2046 - categorical_accuracy: 0.4782 - val_loss: 1.4473 - val_categorical_accuracy: 0.4440\n",
      "Epoch 8/60\n",
      "31/31 [==============================] - 40s 1s/step - loss: 1.1020 - categorical_accuracy: 0.5468 - val_loss: 1.5904 - val_categorical_accuracy: 0.4149\n",
      "Epoch 9/60\n",
      "31/31 [==============================] - 40s 1s/step - loss: 1.0179 - categorical_accuracy: 0.5738 - val_loss: 1.5921 - val_categorical_accuracy: 0.4066\n",
      "Epoch 10/60\n",
      "31/31 [==============================] - 38s 1s/step - loss: 0.9843 - categorical_accuracy: 0.5863 - val_loss: 1.4372 - val_categorical_accuracy: 0.4398\n",
      "Epoch 11/60\n",
      "31/31 [==============================] - 38s 1s/step - loss: 0.8067 - categorical_accuracy: 0.6590 - val_loss: 1.4995 - val_categorical_accuracy: 0.4232\n",
      "Epoch 12/60\n",
      "31/31 [==============================] - 38s 1s/step - loss: 0.7966 - categorical_accuracy: 0.6580 - val_loss: 1.5582 - val_categorical_accuracy: 0.4689\n",
      "Epoch 13/60\n",
      "31/31 [==============================] - 38s 1s/step - loss: 0.7696 - categorical_accuracy: 0.6819 - val_loss: 1.3545 - val_categorical_accuracy: 0.5062\n",
      "Epoch 14/60\n",
      "31/31 [==============================] - 39s 1s/step - loss: 0.6715 - categorical_accuracy: 0.7318 - val_loss: 1.4305 - val_categorical_accuracy: 0.5104\n",
      "Epoch 15/60\n",
      "31/31 [==============================] - 44s 1s/step - loss: 0.6554 - categorical_accuracy: 0.7328 - val_loss: 1.2548 - val_categorical_accuracy: 0.5560\n",
      "Epoch 16/60\n",
      "31/31 [==============================] - 40s 1s/step - loss: 0.5334 - categorical_accuracy: 0.7786 - val_loss: 1.4450 - val_categorical_accuracy: 0.4896\n",
      "Epoch 17/60\n",
      "31/31 [==============================] - 39s 1s/step - loss: 0.6049 - categorical_accuracy: 0.7609 - val_loss: 1.2014 - val_categorical_accuracy: 0.6432\n",
      "Epoch 18/60\n",
      "31/31 [==============================] - 62s 2s/step - loss: 0.6182 - categorical_accuracy: 0.7599 - val_loss: 1.2755 - val_categorical_accuracy: 0.6183\n",
      "Epoch 19/60\n",
      "31/31 [==============================] - 48s 2s/step - loss: 0.5344 - categorical_accuracy: 0.8025 - val_loss: 1.2447 - val_categorical_accuracy: 0.6266\n",
      "Epoch 20/60\n",
      "31/31 [==============================] - 48s 2s/step - loss: 0.3819 - categorical_accuracy: 0.8586 - val_loss: 0.3726 - val_categorical_accuracy: 0.8589\n",
      "Epoch 21/60\n",
      "31/31 [==============================] - 45s 1s/step - loss: 0.2952 - categorical_accuracy: 0.8919 - val_loss: 0.4435 - val_categorical_accuracy: 0.8299\n",
      "Epoch 22/60\n",
      "31/31 [==============================] - 41s 1s/step - loss: 0.2271 - categorical_accuracy: 0.9200 - val_loss: 0.3598 - val_categorical_accuracy: 0.8589\n",
      "Epoch 23/60\n",
      "31/31 [==============================] - 39s 1s/step - loss: 0.1979 - categorical_accuracy: 0.9356 - val_loss: 0.9155 - val_categorical_accuracy: 0.7469\n",
      "Epoch 24/60\n",
      "31/31 [==============================] - 37s 1s/step - loss: 0.2789 - categorical_accuracy: 0.8950 - val_loss: 0.8550 - val_categorical_accuracy: 0.7303\n",
      "Epoch 25/60\n",
      "31/31 [==============================] - 42s 1s/step - loss: 0.3005 - categorical_accuracy: 0.8919 - val_loss: 0.6283 - val_categorical_accuracy: 0.7884\n",
      "Epoch 26/60\n",
      "31/31 [==============================] - 44s 1s/step - loss: 0.2381 - categorical_accuracy: 0.9210 - val_loss: 0.7653 - val_categorical_accuracy: 0.7469\n",
      "Epoch 27/60\n",
      "31/31 [==============================] - 42s 1s/step - loss: 0.2212 - categorical_accuracy: 0.9220 - val_loss: 0.2975 - val_categorical_accuracy: 0.8755\n",
      "Epoch 28/60\n",
      "31/31 [==============================] - 41s 1s/step - loss: 0.2238 - categorical_accuracy: 0.9075 - val_loss: 0.8382 - val_categorical_accuracy: 0.7178\n",
      "Epoch 29/60\n",
      "31/31 [==============================] - 42s 1s/step - loss: 0.1207 - categorical_accuracy: 0.9626 - val_loss: 1.2009 - val_categorical_accuracy: 0.6888\n",
      "Epoch 30/60\n",
      "31/31 [==============================] - 38s 1s/step - loss: 0.1942 - categorical_accuracy: 0.9356 - val_loss: 0.8320 - val_categorical_accuracy: 0.7759\n",
      "Epoch 31/60\n",
      "31/31 [==============================] - 46s 1s/step - loss: 0.3314 - categorical_accuracy: 0.8836 - val_loss: 0.3405 - val_categorical_accuracy: 0.8755\n",
      "Epoch 32/60\n",
      "31/31 [==============================] - 39s 1s/step - loss: 0.1375 - categorical_accuracy: 0.9501 - val_loss: 0.5326 - val_categorical_accuracy: 0.8257\n",
      "Epoch 33/60\n",
      "31/31 [==============================] - 39s 1s/step - loss: 0.1987 - categorical_accuracy: 0.9324 - val_loss: 0.5996 - val_categorical_accuracy: 0.8008\n",
      "Epoch 34/60\n",
      "31/31 [==============================] - 39s 1s/step - loss: 0.0946 - categorical_accuracy: 0.9626 - val_loss: 0.6118 - val_categorical_accuracy: 0.8257\n",
      "Epoch 35/60\n",
      "31/31 [==============================] - 39s 1s/step - loss: 0.0825 - categorical_accuracy: 0.9699 - val_loss: 0.3575 - val_categorical_accuracy: 0.8838\n",
      "Epoch 36/60\n",
      "31/31 [==============================] - 39s 1s/step - loss: 0.1332 - categorical_accuracy: 0.9657 - val_loss: 1.0779 - val_categorical_accuracy: 0.7386\n",
      "Epoch 37/60\n",
      "31/31 [==============================] - 39s 1s/step - loss: 0.1552 - categorical_accuracy: 0.9522 - val_loss: 0.6086 - val_categorical_accuracy: 0.8382\n",
      "Epoch 38/60\n",
      "31/31 [==============================] - 39s 1s/step - loss: 0.2749 - categorical_accuracy: 0.9116 - val_loss: 0.6240 - val_categorical_accuracy: 0.8133\n",
      "Epoch 39/60\n",
      "31/31 [==============================] - 40s 1s/step - loss: 0.1719 - categorical_accuracy: 0.9418 - val_loss: 0.4630 - val_categorical_accuracy: 0.8548\n",
      "Epoch 40/60\n",
      "31/31 [==============================] - 44s 1s/step - loss: 0.0896 - categorical_accuracy: 0.9678 - val_loss: 0.4118 - val_categorical_accuracy: 0.8963\n",
      "Epoch 41/60\n",
      "31/31 [==============================] - 40s 1s/step - loss: 0.1092 - categorical_accuracy: 0.9584 - val_loss: 0.5076 - val_categorical_accuracy: 0.8257\n",
      "Epoch 42/60\n",
      "31/31 [==============================] - 40s 1s/step - loss: 0.0900 - categorical_accuracy: 0.9699 - val_loss: 1.3684 - val_categorical_accuracy: 0.6805\n",
      "Epoch 43/60\n",
      "31/31 [==============================] - 39s 1s/step - loss: 0.0806 - categorical_accuracy: 0.9761 - val_loss: 0.5305 - val_categorical_accuracy: 0.8340\n",
      "Epoch 44/60\n",
      "31/31 [==============================] - 39s 1s/step - loss: 0.1223 - categorical_accuracy: 0.9647 - val_loss: 0.7291 - val_categorical_accuracy: 0.7884\n",
      "Epoch 45/60\n",
      "31/31 [==============================] - 39s 1s/step - loss: 0.1215 - categorical_accuracy: 0.9543 - val_loss: 0.5798 - val_categorical_accuracy: 0.8423\n",
      "Epoch 46/60\n",
      "31/31 [==============================] - 72s 2s/step - loss: 0.0906 - categorical_accuracy: 0.9709 - val_loss: 0.3865 - val_categorical_accuracy: 0.8755\n",
      "Epoch 47/60\n",
      "31/31 [==============================] - 1102s 37s/step - loss: 0.2031 - categorical_accuracy: 0.9366 - val_loss: 0.4813 - val_categorical_accuracy: 0.8589\n",
      "Epoch 48/60\n",
      "31/31 [==============================] - 2905s 77s/step - loss: 0.1063 - categorical_accuracy: 0.9626 - val_loss: 0.5265 - val_categorical_accuracy: 0.8797\n",
      "Epoch 49/60\n",
      "31/31 [==============================] - 41s 1s/step - loss: 0.0723 - categorical_accuracy: 0.9751 - val_loss: 0.4081 - val_categorical_accuracy: 0.8672\n",
      "Epoch 50/60\n",
      "31/31 [==============================] - 40s 1s/step - loss: 0.1926 - categorical_accuracy: 0.9439 - val_loss: 1.2804 - val_categorical_accuracy: 0.7012\n",
      "Epoch 51/60\n",
      "31/31 [==============================] - 40s 1s/step - loss: 0.1231 - categorical_accuracy: 0.9605 - val_loss: 0.6513 - val_categorical_accuracy: 0.8008\n",
      "Epoch 52/60\n",
      "31/31 [==============================] - 36s 1s/step - loss: 0.1978 - categorical_accuracy: 0.9407 - val_loss: 1.2082 - val_categorical_accuracy: 0.6805\n",
      "Epoch 53/60\n",
      "31/31 [==============================] - 39s 1s/step - loss: 0.1195 - categorical_accuracy: 0.9740 - val_loss: 0.5550 - val_categorical_accuracy: 0.8133\n",
      "Epoch 54/60\n",
      "31/31 [==============================] - 38s 1s/step - loss: 0.0321 - categorical_accuracy: 0.9886 - val_loss: 0.3059 - val_categorical_accuracy: 0.9087\n",
      "Epoch 55/60\n",
      "31/31 [==============================] - 38s 1s/step - loss: 0.0387 - categorical_accuracy: 0.9844 - val_loss: 0.3282 - val_categorical_accuracy: 0.9129\n",
      "Epoch 56/60\n",
      "31/31 [==============================] - 38s 1s/step - loss: 0.0886 - categorical_accuracy: 0.9647 - val_loss: 0.7157 - val_categorical_accuracy: 0.8423\n",
      "Epoch 57/60\n",
      "31/31 [==============================] - 38s 1s/step - loss: 0.1231 - categorical_accuracy: 0.9615 - val_loss: 0.3808 - val_categorical_accuracy: 0.8838\n",
      "Epoch 58/60\n",
      "31/31 [==============================] - 38s 1s/step - loss: 0.1298 - categorical_accuracy: 0.9595 - val_loss: 0.6662 - val_categorical_accuracy: 0.8174\n",
      "Epoch 59/60\n",
      "31/31 [==============================] - 38s 1s/step - loss: 0.0582 - categorical_accuracy: 0.9823 - val_loss: 0.2122 - val_categorical_accuracy: 0.9544\n",
      "Epoch 60/60\n",
      "31/31 [==============================] - 38s 1s/step - loss: 0.0176 - categorical_accuracy: 0.9927 - val_loss: 0.2697 - val_categorical_accuracy: 0.9253\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29c4fec80>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_model.fit(X_train,to_categorical(y_train) ,epochs=60,\n",
    "                            validation_data=(X_valid, to_categorical(y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 2s 235ms/step - loss: 0.2697 - categorical_accuracy: 0.95436\n",
      "\n",
      "[0.26972249150276184, 0.9543612077713013]\n"
     ]
    }
   ],
   "source": [
    "main_model.evaluate(X_test, to_categorical(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0910852971f271316cedd65fb9a6fba74ce8d38f423ea6cfef93e6992f0711c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
